{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=center src=\"https://csc.web.cern.ch/sites/csc.web.cern.ch/files/CSC-LightBackground-200px.png\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Exercise 3: The Map-Reduce Pattern of Parallelism*\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise aims to illustrate the characteristics of the Map-Reduce parallel pattern by means of the Spark programming model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left src=\"http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png\" width=150/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Spark](http://spark.apache.org) is a framework for parallel distributed data processing on a set of commodity machines. It frees the user from duties like task scheduling, data transfer and fault tolerance, so that they can focus on programming.\n",
    "\n",
    "The Spark programming model uses the Map-Reduce model as a basis, simplifying its usage and extending it with more functionality. The following are two basic concepts in Spark:\n",
    "* `Resilient Distributed Datasets (RDDs)`: an RDD is a distributed collection of items. The collection is divided in partitions, and these partitions are the units of parallelism.\n",
    "* `Transformations and actions`: the user can apply either transformations or actions on RDDs. Transformations are `lazy`, they are not applied right away, unlike actions. In other words, one can apply a chain of transformations to an RDD, but only when appending an action to that chain the whole computation graph will start executing. `map` is an example of a transformation, while `reduce` is an action. However, Spark is not limited to just map and reduce: it offers a rich variety of functional-style operators. Although we will not use all those operators in this exercise, you can find a complete list [here](http://spark.apache.org/docs/latest/programming-guide.html).\n",
    "\n",
    "As for the language, Spark programs can be developed in Scala, Java and Python. The latter will be used in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Map-Reduce on a collection of elements\n",
    "\n",
    "Map-Reduce is a combination of two patterns of parallelism:\n",
    "* `Map`: given a collection of elements of type $X$, apply a function $F_{map}$ to each element that returns a single element of type $Y$.\n",
    "* `Reduce`: given a collection of elements of type $Y$, combine its elements into an element of type $Y$ using a function $F_{reduce}$.\n",
    "\n",
    "These two patterns can be chained to obtain a single result from a collection of elements:\n",
    "\n",
    "$(X_{1},~X_{2},~...,~X_{n})~\\xrightarrow{F_{map}}~(Y_{1},~Y_{2},~...,~Y_{n})~\\xrightarrow{F_{reduce}}~Y_{result}$\n",
    "\n",
    "Spark implements these patterns via two operators, [map](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) and [reduce](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce). Both receive a function as parameter; in the case of `map`, the function must receive **one** parameter of type $X$ and return a value of type $Y$; regarding `reduce`, the function must receive **two** parameters of type $Y$ and return a value of type $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First example\n",
    "\n",
    "Let's start with a simple example of how to run a Map-Reduce chain with Spark.\n",
    "\n",
    "First we will perform some initialisation of the Spark library. In particular, we will create a `SparkContext` object that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create our first RDD, i.e. a collection of elements in Spark. In this case, the collection is a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 4, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now `map` a function to our collection to increment each of its elements. We will first define such function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def increment(x):\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the `increment` function to every element in our collection via the `map` operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left src=\"https://cdn.rawgit.com/jkthompson/pyspark-pictures/master/images/pyspark-page3.svg\" width=500 height=250 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incrementedRDD = rdd.map(increment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually check the incremented values with the [collect](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) function of Spark. Note that with the statement above you just *scheduled* a calculation which is not executed by the Spark runtime: `map` is indeed a *transformation*. The actual work is triggered by an *action*, such as `collect` or `reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incrementedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we would like to obtain the sum of all the numbers in the collection. For that purpose, we can use `reduce` and a function that returns the sum of two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's `reduce` functionality will apply `add` to our collection, adding its elements two by two until calculating the total sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left src=\"https://cdn.rawgit.com/jkthompson/pyspark-pictures/master/images/pyspark-page23.svg\" width=500 height=250 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalSum = incrementedRDD.reduce(add)\n",
    "totalSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also do the same calculation in a single Map-Reduce chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.map(increment).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise to complete\n",
    "\n",
    "Now that you became familiar with the Map-Reduce pattern and its implementation in Spark, we propose you an exercise to apply what you just learned to a different problem.\n",
    "\n",
    "In this case, our collection will be a list of town names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = sc.parallelize(['Madrid', 'Geneva', 'Barcelona', 'Milano', 'Mamungkukumpurangkuntjunya'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we would like to know the length of the longest name in our list.\n",
    "\n",
    "First, we need a function that receives a string and returns its length (hint: you can use the [len](https://docs.python.org/2/library/functions.html#len) Python function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(s):\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define another function that, given two numbers, returns the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maximum(x, y):\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, combine the two functions you just defined in a Map-Reduce chain to obtain the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Map-Reduce on text files: Word Count\n",
    "\n",
    "A typical real-life example where the Map-Reduce pattern is applied is the processing of text files like, for instance, logs generated by a web server.\n",
    "\n",
    "In this second part of the exercise, you will process a text file and count the occurrences of each word in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data\n",
    "First we will download the input file for this exercise, which corresponds to the Complete Works of William Shakespeare, offered by [Project Gutenberg](http://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl  -o 100.zip https://swanserver.web.cern.ch/swanserver/csc/100.zip\n",
    "!unzip -o 100.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use Spark to create a collection out of this text file. In the collection, every line of the file will be an element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile('100.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textFile` is a collection where each element is a string corresponding to a line in the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing new operations\n",
    "\n",
    "The Map-Reduce chain to count the ocurrences of each word in the file is a bit more complicated than what we have seen so far. The chain will contain **two** map operations and one reduce. In addition, you will have to use some new operators, which are a slight variation of the `map` and `reduce` you already know:\n",
    "\n",
    "* [flatMap](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap): if the user function $F_{map}$ returns a collection of values instead of single value, `flatMap` decomposes the collection into individual elements in the final result.\n",
    "\n",
    "<img align=left src=\"https://cdn.rawgit.com/jkthompson/pyspark-pictures/master/images/pyspark-page4.svg\" width=500 height=250/>\n",
    "\n",
    "<br clear=\"all\">\n",
    "\n",
    "* [reduceByKey](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey): like in the case of `reduce`, the function $F_{reduce}$ passed as parameter of `reduceByKey` receives two elements of type $Y$ and returns an element of the same type. However, `reduceByKey` is applied on collections of key-value pairs, that is, $( ~(key_{1},val_{x}),~(key_{2},val_{y}),~(key_{1},val_{z}),~...~)$, and the output is the reduction of the values for each key $( ~(key_{1}, red_{1}),~(key_{2},red_{2}),~...~)$.\n",
    "\n",
    "<img align=left src=\"https://cdn.rawgit.com/jkthompson/pyspark-pictures/master/images/pyspark-page44.svg\" width=500 height=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Let's now define the functions that will be applied in the Map-Reduce chain. First, you will need a function that splits a line of the text file in a list of words (hint: you can use the [split](https://docs.python.org/2/library/stdtypes.html#str.split) Python function and return words with non alphanumeric characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitWords(line):\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function you will need receives a word and returns a key-value pair, where the key is the word and the value is 1 (its partial count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count(word):\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will need a function that receives two numbers and returns their sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(x, y):\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Reduce chain\n",
    "\n",
    "To finish this exercise, combine the three functions you just defined in a chain to get the counts per word in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordCounts = textFile# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you got it right..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordCounts.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
